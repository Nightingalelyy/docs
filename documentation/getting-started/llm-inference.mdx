---
title: "LLM inference"
og:title: 'LLM inference'
---

---

## Logging, gateway, or tracing?

<iframe
  className="w-full aspect-video rounded-xl"
  src="https://www.youtube.com/embed/QGrUM_H3q_k"
  title="LLM logging vs AI gateway vs Agent tracing"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
  allowFullScreen={true}
></iframe>

---

## LLM logging

> **Call the LLM yourself**, and we help you monitor those calls.

If you already have LLM API calls in your code and just want observability (prompt, response, latency, user tracking, etc.), use the **Logging API**.

- No latency
- Works with any LLM provider
- Fast setup with a single HTTP call

ðŸ‘‰ [Quickstart](/documentation/products/logs/quickstart)

---

## LLM gateway

> We help you to **handle LLM calls and everything.**

The Gateway is the easiest way to connect to major LLM providers. We can help you to handle rerouting, retries, load-balancing, caching, fallbacks, and etc..

- 50 - 150ms latency
- Support 250+ models
- Built-in cost tracking, retries, observability

ðŸ‘‰ [Quickstart](/get-started/quickstart/gateway)

---

## Agent tracing

> Track your agent workflows step by step

Perfect for multi-step chains, working with popular AI agent frameworks.

- Native support for [OpenAI Agent SDK](https://platform.openai.com/), [Mastra](https://www.mastra.dev/), and more  
- Custom step tracing with our SDK
- Visualize execution trees, tools, retries, state changes

ðŸ‘‰ [Quickstart](/documentation/products/traces/quickstart)

---

## Prompt management and evaluations

> Isolate prompts from code, collaborate with prompt engineers, and iterate faster.

- Version control for prompts  
- Human + LLM evaluation pipelines  
- Visual scoreboards, test sets, experiments

ðŸ‘‰ [Quickstart](/documentation/products/prompt_management/quickstart)

---

## Need help?
[Join our discord](https://discord.com/invite/KEanfAafQQ) â€” we'll help you as best we can.