---
title: Rate limit for users
description: The user-level rate limit helps you control the LLM usage of each user.
---

<Note>
This feature is only available for AI gateway users.
</Note>

The user-level rate limit helps you control the LLM usage of each user. You can set a rate limit of **requests per minute** for each user and we will block the API calls that exceed the limit.

## Why user-level rate limit?
- Prevent random users from abusing your system
- Control the cost of your LLM usage

## How to set user-level rate limit
```json
"customer_params": {
        "customer_identifier": "xxxx", // The user you want to set the rate limit for
        "rate_limit": 100 // The rate limit of the user, requests per minute
    },
```

### Detailed example

<Tabs>
<Tab title="AI gateway">
<CodeGroup>
```python Python {14}
import requests
def demo_call(input, 
              model="gpt-4o-mini",
              token="YOUR_RESPAN_API_KEY"
              ):
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {token}',
    }

    data = {
        'model': model,
        'messages': [{'role': 'user', 'content': input}],
        "customer_params": {"customer_identifier": "xxxx", "rate_limit": 100}
    }

    response = requests.post('https://api.respan.ai/api/chat/completions', headers=headers, json=data)
    return response

messages = "Say 'Hello World'"
print(demo_call(messages).json())
```


```TypeScript TypeScript {10}
fetch('https://api.respan.ai/api/chat/completions', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
    'Authorization': 'Bearer YOUR_RESPAN_API_KEY'
  },
    body: JSON.stringify({
        model: 'gpt-4o-mini',
        messages: [{role: 'user', content: "Say 'Hello World'"}],
        "customer_params": {"customer_identifier": "xxxx", "rate_limit": 100}
    })
})
.then(response => response.json())
.then(data => console.log(data));
```
</CodeGroup>
</Tab>
<Tab title="OpenAI SDK">

<CodeGroup>
```python Python {11}
from openai import OpenAI

client = OpenAI(
    base_url="https://api.respan.ai/api/",
    api_key=YOUR_RESPAN_API_KEY,
)

response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role":"user", "content":"Tell me a long story"}],
    extra_body={"customer_params": {"customer_identifier": "xxxx", "rate_limit": 100}}
)
```

```TypeScript TypeScript {13}
import { OpenAI } from "openai";

const client = new OpenAI({
  baseURL: "https://api.respan.ai/api",
  apiKey: process.env.RESPAN_API_KEY,
});

const response = await client.chat.completions
  .create({
    messages: [{ role: "user", content: "Say this is a test" }],
    model: "gpt-4o-mini",
    // @ts-expect-error
    customer_params: { customer_identifier: "xxxx", rate_limit: 100 },
  })
  .asResponse();

console.log(await response.json());
```


</CodeGroup>

</Tab>

<Tab title= "Other SDKs">
We also support user-level rate limit in other SDKs or languages, please check out our [integration section](/documentation/admin/llm_provider_keys) for more information.
</Tab>
</Tabs>