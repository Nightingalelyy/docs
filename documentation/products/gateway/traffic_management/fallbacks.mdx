---
title: "Fallback models"
description: "This is the guide for how to set up fallbacks in Respan"
---

Respan will catch any errors occurring in a request to model and fall back to the list of models you specified in the `fallback_models` field. This is useful to avoid downtime and ensure that your application is always available.

See all Respan params [here](/api-endpoints/develop/gateway/chat-completions#respan-parameters).

## Set up fallbacks
Fallback models can be set up in the UI or in the code. You can add multiple fallback models to the list, we will try each model in the order they are listed.
### Set up fallbacks from the UI
Go to Settings -> [Fallback](https://platform.respan.ai/platform/api/fallback) -> Click on `Add fallback models` -> Select the models you want to add as fallbacks.

You can drag and drop the models to reorder them. The order of the models in the list is the order in which they will be tried.
<Frame>
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/llm-proxy/fallback.png" alt="Fallback Page"/>
</Frame>

### Set up fallbacks in code
<Tabs>
<Tab title="OpenAI Python SDK">
```python {14}
from openai import OpenAI

client = OpenAI(
    base_url="https://api.respan.ai/api/",
    api_key="YOUR_RESPAN_API_KEY",
)

response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {"role": "user", "content": "Tell me a long story"}
    ],
    extra_body={
        "fallback_models": ["claude-3-5-sonnet-20240620", "gemini-1.5-pro-002"]
    }
)
```
</Tab>
<Tab title="OpenAI TypeScript SDK">
In OpenAI TypeScript SDK, you should add a `// @ts-expect-error` before the `fallback_models` field.
```typescript {12}
import { OpenAI } from "openai";

const client = new OpenAI({
  baseURL: "https://api.respan.ai/api",
  apiKey: "YOUR_RESPAN_API_KEY",
});

const response = await client.chat.completions
  .create({
    messages: [{ role: "user", content: "Say this is a test" }],
    model: "gpt-4o-mini",
    // @ts-expect-error
    fallback_models: ["claude-3-5-sonnet-20240620", "gemini-1.5-pro-002"]
  })
  .asResponse();

console.log(await response.json());
```
</Tab>
<Tab title="Standard API">
```python {14}
import requests
def demo_call(input, 
              model="gpt-4o-mini",
              token="YOUR_RESPAN_API_KEY",
              ):
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {token}',
    }

    data = {
        'model': model,
        'messages': [{'role': 'user', 'content': input}],
        'fallback_models': ["claude-3-5-sonnet-20240620", "gemini-1.5-pro-002"]
    }

    response = requests.post('https://api.respan.ai/api/chat/completions', headers=headers, json=data)
    return response

messages = "Say 'Hello World'"
print(demo_call(messages).json())
```
</Tab>
<Tab title="Other SDKs">
We also support adding credentials in other SDKs or languages, please check out our [integration section](/documentation/admin/llm_provider_keys) for more information.
</Tab>
</Tabs>
{/* ## Automatic fallbacks

We use Saftey net to automatically fallback to a system assigned model if fallback models are not responding or specified.

Turn on the Safety net [here](https://platform.respan.ai/platform/api/fallback). */}

## Troubleshooting
- You turned off Fallbacks in the UI
- You uploaded some invalid [`provider_credentials`](/documentation/admin/llm_provider_keys)
- The models you specified in the Fallbacks list are not available
- The provider of the model does not return a response (so we can't detect if it's down or not)

{/* ## Fallback logic with retries

Below is the pseudo-code for the fallback logic with retries:

```python
warnings = {}
model = "model_in_request_that_will_fail"
respan_native_failover = respan_models_data[model]["fallback"] # A slice of data from our database for the native failover
# Native failovers usually are the same models, but just from different providers, they are optimally ranked to reduce cost and latency
fallback_models = [*respan_native_failover, "model1", "model2"]

# Respan will filter away models that do not meet the hard requirements of the request (function_calling support, etc.)
# To save time from unnecessary retries
fallback_models = filter_by_hard_requirements(request, fallback_models)

for model in fallback_models:
  for _ in range(retries):
    try:
        response = generate_response_with_load_balance(model)
        break
    except Exception as e:
        error_code, error_message = respan_exception_handler(e)
        warnings[model] = error_message
raise Exception("All models failed", warnings)
```

- To learn more about how `generate_response_with_load_balance` works, see the [Load Balancing](/platform-features/load-balancing) section.
- To learn more about how `filter_by_hard_requirements` works, see the [Model Filtering](/platform-features/model-filtering) section. */}