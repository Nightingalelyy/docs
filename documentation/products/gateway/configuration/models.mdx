---
title: "Supported models"
description: "Call 200+ LLMs with a single OpenAI compatible format"
---


## Choose a model
After you integrate the LLM proxy, you can choose a model from the [Models page](https://platform.respan.ai/platform/models). In this page, you can see each model's description, pricing, and other metrics, which helps you choose the best model for your use case.
<Frame>
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/llm-proxy/models.png"/>
</Frame>

### Model family
You can also click an exact model to see it's model family, which is a group of models that hosted by the different LLM providers.
<Frame>
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/llm-proxy/model-details.png"/>
</Frame>

### Integration code
If you have already integrated the LLM proxy, you can click the `Code` button to copy the integration code with the language you are using.
<Frame>
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/llm-proxy/model-code.png"/>
</Frame>

## Call models in different frameworks
<Tabs>
<Tab title="OpenAI Python SDK">
```python {9}
from openai import OpenAI

client = OpenAI(
    base_url="https://api.respan.ai/api/",
    api_key="YOUR_RESPAN_API_KEY",
)

response = client.chat.completions.create(
    model="claude-3-5-haiku-20241022",
    messages=[
        {"role": "user", "content": "Tell me a long story"}
    ]
)
```
</Tab>
<Tab title="OpenAI TypeScript SDK">
Here is an example of how to disable logging in the OpenAI TypeScript SDK. In OpenAI TypeScript SDK, you should add a `// @ts-expect-error` before the disable_log field.
```typescript {11}
import { OpenAI } from "openai";

const client = new OpenAI({
  baseURL: "https://api.respan.ai/api",
  apiKey: "YOUR_RESPAN_API_KEY",
});

const response = await client.chat.completions
  .create({
    messages: [{ role: "user", content: "Say this is a test" }],
    model: "claude-3-5-sonnet-20241022"
  })
  .asResponse();

console.log(await response.json());
```
</Tab>
<Tab title="Standard API">
```python  {3}
import requests
def demo_call(input, 
              model="groq/llama-3.3-70b-specdec",
              token="YOUR_RESPAN_API_KEY",
              ):
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {token}',
    }

    data = {
        'model': model,
        'messages': [{'role': 'user', 'content': input}],
    }

    response = requests.post('https://api.respan.ai/api/chat/completions', headers=headers, json=data)
    return response

messages = "Say 'Hello World'"
print(demo_call(messages).json())
```
</Tab>
<Tab title="Other SDKs">
We also support adding credentials in other SDKs or languages, please check out our [integration section](/documentation/admin/llm_provider_keys) for more information.
</Tab>
</Tabs>