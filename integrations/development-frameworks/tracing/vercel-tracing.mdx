---
title: Vercel AI SDK
description: Learn how to integrate Respan tracing with Vercel AI SDK to monitor and analyze your AI application performance. Step-by-step guide for setting up environment variables and creating traced workflows.
og:title: 'Agent tracing with Vercel AI SDK'
---
This tutorial shows how to set up Respan tracing with [Next.js](https://nextjs.org/) and the [Vercel AI SDK](https://ai-sdk.dev/) to monitor and trace your AI-powered applications.


## Start with the pre-built example

We have built a pre-built example for you to get started quickly. You can find the example [here](https://github.com/Respan/respan-example-projects/tree/main/vercel_ai_next_openai).

Get up and running quickly with our pre-configured example:

```bash
npx create-next-app --example https://github.com/Respan/respan-example-projects/tree/main/vercel_ai_next_openai my-respan-app
```

```bash
yarn create next-app --example https://github.com/Respan/respan-example-projects/tree/main/vercel_ai_next_openai my-respan-app
```
```bash
pnpm create next-app --example https://github.com/Respan/respan-example-projects/tree/main/vercel_ai_next_openai my-respan-app
```

Then:

1. Add your API keys to `.env.local` (see Step 3 below)
2. Run `yarn dev` to start the development server
3. Start chatting and check your [Respan dashboard](https://platform.respan.ai/platform/dashboard)


## Start from scratch

You can also follow the Step-by-Step Tutorial to build your own application.

If you want to understand the setup process or add Respan tracing to an existing project, follow the tutorial below.

Execute `create-next-app` with npm, Yarn, or pnpm to bootstrap the base example:

```bash
npx create-next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai next-openai-app
```

```bash
yarn create next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai next-openai-app
```

```bash
pnpm create next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai next-openai-app
```

To run the base example locally you need to:

1. Sign up at [OpenAI's Developer Platform](https://platform.openai.com/signup).
2. Go to [OpenAI's dashboard](https://platform.openai.com/dashboard) and create an API KEY.
3. If you choose to use external files for attachments, then create a [Vercel Blob Store](https://vercel.com/docs/storage/blobs).
4. Set the required environment variable as the token value as shown the example env file but in a new file called `.env.local`
5. Run `pnpm install` to install the required dependencies.
6. Run `pnpm dev` to launch the development server.


## Respan Telemetry Setup

Now let's add Respan tracing to monitor your AI application's performance and usage.

### Step 1: Install Respan Exporter

Install the Respan exporter package:

```bash
npm install @respan/exporter-vercel
```

```bash
yarn add @respan/exporter-vercel
```

```bash
bun add @respan/exporter-vercel
```

```bash
pnpm add @respan/exporter-vercel
```

### Step 2: Set up OpenTelemetry Instrumentation

Next.js supports OpenTelemetry instrumentation out of the box. Following the Next.js OpenTelemetry guide, create an instrumentation.ts file in your project root:

Install vercel's opentelemetry instrumentation:
```bash
yarn add @vercel/otel
```

Create instrumentation.ts in the root (where package.json lives) of your project

```typescript instrumentation.ts
import { registerOTel } from "@vercel/otel";
import { RespanExporter } from "@respan/exporter-vercel";

export function register() {
  registerOTel({
    serviceName: "next-app",
    traceExporter: new RespanExporter({ // <---- Use Respan exporter as the custom exporter
      apiKey: process.env.RESPAN_API_KEY,
      baseUrl: process.env.RESPAN_BASE_URL,
      debug: true
    }),
  });
}
```

### Step 3: Configure Environment Variables
Add your Respan credentials to your `.env.local` file:

```bash .env.local
# OpenAI API Key (existing)
OPENAI_API_KEY=your_openai_api_key_here

# Respan Configuration
RESPAN_API_KEY=your_respan_api_key_here
RESPAN_BASE_URL=https://api.respan.ai  # Optional: defaults to Respan API
```

### Step 4: Enable Telemetry in API Routes
In your API route files (e.g., `app/api/chat/route.ts`), enable telemetry by adding the `experimental_telemetry` option to your AI SDK functions:

```typescript app/api/chat/route.ts
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai';

// Allow streaming responses up to 30 seconds
export const maxDuration = 30;

export async function POST(req: Request) {
  // Extract the `messages` from the body of the request
  const { messages, id } = await req.json();

  console.log('chat id', id); // can be used for persisting the chat
  const createHeader = () => {
    return {
      'X-Data-Respan-Params': Buffer.from(JSON.stringify({
        prompt_unit_price: 100000
      })).toString('base64')
    }
  }

  // Call the language model
  const result = streamText({
    model: openai('gpt-4o'),
    messages,
    async onFinish({ text, toolCalls, toolResults, usage, finishReason }) {
      // implement your own logic here, e.g. for storing messages
      // or recording token usage
    },
    experimental_telemetry: {
      isEnabled: true,
      metadata: {
        customer_identifier: "customer_from_metadata",
        prompt_unit_price: 100000
      },
      headers: createHeader()
    }
  });

  // Respond with the stream
  return result.toDataStreamResponse();
}
```

Here is the example on how to set parameters for tracing when using vercel AI sdk: [Github](https://github.com/Respan/respan-example-projects/tree/main/vercel_ai_next_openai/app/api/chat)


### Step 5: Test Your Setup
1. Start your development server:

```bash
pnpm dev
```

```bash
yarn dev
```

There might be broken dependencies in the @vercel/otel package, simply install them if you see them:

```bash
yarn add @opentelementry/api-logs
```

If this fails, make sure you are use the right version of package manager

```json package.json
"packageManager" : "yarn@4.9.2"
```
And try again

2. Make some chat requests through your application

3. Check your Respan application:
- Go to Signals -> Traces
- Check the log that is traced

## What Gets Traced
With this setup, Respan will automatically capture:

- **AI Model Calls**: All calls to OpenAI models through the AI SDK
- **Request/Response Data**: Input messages and generated responses
- **Token Usage**: Input and output token counts for cost tracking
- **Performance Metrics**: Latency and throughput data
- **Error Tracking**: Failed requests and error details
- **Custom Metadata**: Any additional context you want to track

## Learn More
To learn more about the technologies used in this tutorial:

- [AI SDK docs](https://ai-sdk.dev/) - comprehensive AI SDK documentation
- [Next.js OpenTelemetry Guide](https://nextjs.org/docs/app/guides/open-telemetry) - official Next.js telemetry documentation
- [Vercel AI Playground](https://ai-sdk.dev/playground) - test AI models interactively
- [OpenAI Documentation](https://platform.openai.com/docs/api-reference) - learn about OpenAI features and API
- [Next.js Documentation](https://nextjs.org/docs/app) - learn about Next.js features and API






{/* ## Prerequisites
- Vercel AI SDK
- [Respan API key](https://platform.respan.ai/platform/api/api-keys)

## Steps

<Steps>
<Step title="Install Respan SDK"> 
  ```bash
npm install @respan/tracing
  ```
</Step>
<Step title="Add an instrumentation file to the root of your project">
You should add an instrumentation file to the root of your project. For example, `instrumentation.ts`. Same level as `.env` or `next.config.js` file.

Then add the `RespanExporter` as the trace exporter

```typescript root-of-project/instrumentation.js
import { RespanExporter } from "@respan/tracing";
import { registerOTel } from "@vercel/otel";
// This file is a special Next.js file that is loaded on startup
export async function register() {
  try {
    // Only load the instrumentation in the Node.js environment (not in edge runtime or client)
    registerOTel({
      serviceName: "respan-example",
      traceExporter: new RespanExporter({
        //   baseUrl: "https://api.respan.ai", # This will be the default base URL. enterprise customer should change it
        apiKey: process.env.RESPAN_API_KEY_TEST,
      }),
    });
  } catch (error) {
    console.error("Failed to load instrumentation:", error);
  }
}
```
</Step>
<Step title="(optional) Configure next.config.js">
**If you are using Next.js, please do the following:**
1. Add node-loader package
```bash
npm install node-loader
```
2. Add the following to your `next.config.js` file:
```typescript next.config.js
const nextConfig = {
webpack: (config, { isServer }) => {
  config.module.rules.push({
    test: /\.node$/,
    loader: "node-loader",
  });
  if (isServer) {
    config.ignoreWarnings = [{ module: /opentelemetry/ }];
  }
  // In this block, we will ignore/browserify some node dependencies
     config.resolve.alias = {
      ...config.resolve.alias,
      zlib: require.resolve("browserify-zlib"),
    };
    config.resolve.fallback = {
      ...config.resolve.fallback,
      tls: false,
      fs: false,
      http: false,
      https: false,
      http2: false,
      net: false,
      dns: false,
      os: false,
      path: false,
      stream: false,
    };
  // end of ignoring/browserify node depenencies
  return config;
},
};
```
</Step>
<Step title="Send traces to Respan">
Then, to trace a specific function, simply enable tracing. here is an example of it working on the streamText function:
```typescript
import { openai } from "@ai-sdk/openai";
import { streamText } from "ai";

const result = await streamText({
  model: openai("gpt-4o"),
  messages: convertToCoreMessages(messages),
  // Add this block here
  experimental_telemetry: {
    isEnabled: true,
  }
  // End of add this block
});
```
</Step> 
</Steps> */}



